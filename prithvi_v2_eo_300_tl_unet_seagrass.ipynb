{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/7yl4r/nasa-prithvi-wetlands/blob/main/prithvi_v2_eo_300_tl_unet_seagrass.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "7009d02e",
      "metadata": {
        "id": "7009d02e"
      },
      "outputs": [],
      "source": [
        "# Includes work derived from International Business Machines\n",
        "# `prithvi_v2_eo_300_tl_unet_multitemporal_crop.ipynb``\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# You may obtain a copy of the License at\n",
        "#  http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# All rights reserved for this derived work."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "5ca9883e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ca9883e",
        "outputId": "949a4788-9927-45ca-9895-5ea8a811dbd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: file:///content does not appear to be a Python project: neither 'setup.py' nor 'pyproject.toml' found.\u001b[0m\u001b[31m\n",
            "\u001b[0mObtaining file:///content\n"
          ]
        }
      ],
      "source": [
        "# Bash command to install dependencies from pyproject.toml with `pip``;\n",
        "# ouput is piped to `tail`` to limit length of text printed below.\n",
        "#!pip install -e . | tail -n 7\n",
        "\n",
        "# install directly becausea gcolab does not have access to the pyproject.toml\n",
        "!pip install terratorch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4bacc318390456b",
      "metadata": {
        "id": "b4bacc318390456b"
      },
      "source": [
        "## Overview\n",
        "This notebook focuses on fine-tuning the [Prithvi EO v2.0 model](https://huggingface.co/collections/ibm-nasa-geospatial/prithvi-for-earth-observation-6740a7a81883466bf41d93d6) to classify seagrass.\n",
        "\n",
        "This notebook:\n",
        "1. Is intended to be run on Google Colab.\n",
        "2. Uses Terratorch to fine-tune Prithvi EO v2.0 300m.\n",
        "3. Uses a seagrass patch dataset for fine-tuning derived from the IMaRS SIMM Seagrass Project.\n",
        "4. Uses fine-tuned model for inference.\n",
        "\n",
        "You may want to take this opportunity to double check you're using GPUs on Google Colab before proceeding any further. We have tested this notebook using T4 GPU on the free colab account.\n",
        "\n",
        "## Setup\n",
        "1. Install terratorch\n",
        "\n",
        "To install the necessary packages, execute the cell below. This will take a few minutes. Once the installation process is done, a window will pop up to ask you to restart the session. This is normal and you should proceed to restart using the interface in the pop up window. Once the session has restarted, its important that you ignore the cell below, and go straight to section 0.1.3.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13c25f3b",
      "metadata": {
        "id": "13c25f3b"
      },
      "source": [
        "2. Import dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "2e8c1961-311b-49e0-b6ea-2867b44cb47a",
      "metadata": {
        "id": "2e8c1961-311b-49e0-b6ea-2867b44cb47a"
      },
      "outputs": [],
      "source": [
        "\n",
        "import albumentations\n",
        "import gdown\n",
        "import lightning.pytorch as pl\n",
        "import os\n",
        "import terratorch\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "from terratorch.datamodules import MultiTemporalCropClassificationDataModule\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "c14cbc6b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "c14cbc6b",
        "outputId": "cf2143f6-6d42-4619-aac6-87b17b3fce8e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-1197304907.py, line 5)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-1197304907.py\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    https://drive.google.com/file/d//view?usp=sharing\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "# Download tuning dataset .bz2 from Google Drive and place in `dataset_path`` directory.\n",
        "dataset_path = \"data/tuning_patches\"\n",
        "fname = \"seagrass_tuning_patches.tar.bz2\"\n",
        "gdown.download(\n",
        "    \"https://drive.google.com/uc?id=1VWl2mkTTAG3ih741n3S5UDUj71ygNKDp\",\n",
        "    os.path.join(dataset_path, fname),\n",
        "    quiet=False,\n",
        ")\n",
        "\n",
        "# uzip the downloaded .bz2 file\n",
        "!tar -xvjf {os.path.join(dataset_path, fname)}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "917b65b8e7cd7d65",
      "metadata": {
        "id": "917b65b8e7cd7d65"
      },
      "source": [
        "A tuning dataset should now be in the dataset_path directory.\n",
        "The dataset used is derived from the IMaRS SIMM Seagrasss Project.\n",
        "For methods to generate the patches, see the `py/generate_seagrass_patches.py` script in the repo.\n",
        "\n",
        "\n",
        "\n",
        "The patches need to be split into training and validation sets.\n",
        "The sets are specified using a `.txt` file listing the patch file names for each set.\n",
        "Example `training_chips.txt`:\n",
        "\n",
        "```\n",
        "chip_257_266\n",
        "chip_328_501\n",
        "chip_171_477\n",
        "chip_236_281\n",
        "chip_134_482\n",
        "chip_120_493\n",
        "chip_161_390\n",
        "```\n",
        "\n",
        "The training and validation patches are expected to be in `training_chips` and `validation_chips` subdirectories of the dataset path."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20332275",
      "metadata": {
        "id": "20332275"
      },
      "source": [
        "4. Truncate the dataset for demonstration purposes. Reducing the training dataset to a third of the original size means that model training takes only a few minutes with the resources available during the workshop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4517aab",
      "metadata": {
        "id": "c4517aab"
      },
      "outputs": [],
      "source": [
        "training_data_truncation = 800\n",
        "validation_data_trunction = 4\n",
        "with open(f\"{dataset_path}/training_data.txt\", \"r\") as f:\n",
        "      training_data_list = f.readlines()\n",
        "truncated = training_data_list[0:training_data_truncation]\n",
        "with open(f\"{dataset_path}/training_data.txt\", \"w\") as f:\n",
        "    for i in truncated:\n",
        "        f.write(i)\n",
        "\n",
        "with open(f\"{dataset_path}/validation_data.txt\", \"r\") as f:\n",
        "      training_data_list = f.readlines()\n",
        "truncated = training_data_list[0:validation_data_trunction]\n",
        "with open(f\"{dataset_path}/validation_data.txt\", \"w\") as f:\n",
        "    for i in truncated:\n",
        "        f.write(i)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35ba4d58-8ff6-4f9c-bfb1-a70376f80494",
      "metadata": {
        "id": "35ba4d58-8ff6-4f9c-bfb1-a70376f80494"
      },
      "source": [
        "## Dataset Details\n",
        "\n",
        "Lets start with analysing the dataset.\n",
        "\n",
        "Please note: we have also set the batch_size parameter to 4 and max_epochs to 1 to avoid running out of memory or runtime for users of the free tier colab compute resources. This is enough to demonstrate the entire workflow to the user, but may not result in the best performance. It'll be best to find additional compute resources and increase batch_size and max_epochs in the downloaded config file for improved performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ddd7d83440895e87",
      "metadata": {
        "id": "ddd7d83440895e87"
      },
      "outputs": [],
      "source": [
        "# Each merged sample includes the stacked bands of three time steps\n",
        "!ls \"{dataset_path}/training_chips\" | head"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ecd1487c",
      "metadata": {
        "id": "ecd1487c"
      },
      "outputs": [],
      "source": [
        "# Modify parameters to select the batch size, number of workers, model backbone and epochs ahead of initalizing the MultiTemporalCropClassificationDataModule class for multi-temporal crop classification.\n",
        "batch_size = 4\n",
        "num_workers = 2\n",
        "prithvi_backbone = \"prithvi_eo_v2_300_tl\" # Model can be either prithvi_eo_v1_100, prithvi_eo_v2_300, prithvi_eo_v2_300_tl, prithvi_eo_v2_600, prithvi_eo_v2_600_tl\n",
        "\n",
        "# Total number of epochs the training will run for.\n",
        "max_epochs =  1 # Use 1 epoch for demos\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91025bb8",
      "metadata": {
        "id": "91025bb8"
      },
      "source": [
        "#### Initialise the Datamodules class\n",
        "\n",
        "A Datamodule is a shareable, reusable class that encapsulates all the steps needed to process the data. Here we are using an adjusted dataset class for this dataset (general dataset class could be used as well). To learn more about MultiTemporalCropClassificationDataModule, take a look at the [TerraTorch docs](https://ibm.github.io/terratorch/stable/datamodules/?h=multitemporalcropclassificationdatamodule#terratorch.datamodules.multi_temporal_crop_classification.MultiTemporalCropClassificationDataModule)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import os\n",
        "from collections.abc import Sequence\n",
        "from pathlib import Path\n",
        "from typing import Any\n",
        "\n",
        "import albumentations as A\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import rioxarray\n",
        "import torch\n",
        "from einops import rearrange\n",
        "from matplotlib.figure import Figure\n",
        "from matplotlib.patches import Rectangle\n",
        "from torch import Tensor\n",
        "from torchgeo.datasets import NonGeoDataset\n",
        "from xarray import DataArray\n",
        "\n",
        "from terratorch.datasets.utils import clip_image, default_transform, filter_valid_files, validate_bands\n",
        "\n",
        "\n",
        "class MultiTemporalSeagrassClassification(NonGeoDataset):\n",
        "    \"\"\"NonGeo dataset implementation for seagrass classification\n",
        "\n",
        "    Based onhttps://github.com/terrastackai/terratorch/blob/main/terratorch/datasets/multi_temporal_crop_classification.py\n",
        "    \"\"\"\n",
        "\n",
        "    all_band_names = (\n",
        "        \"COASTAL BLUE\",\n",
        "        \"BLUE\",\n",
        "        \"GREEN I\",\n",
        "        \"GREEN II\",\n",
        "        \"YELLOW\",\n",
        "        \"RED\",\n",
        "        \"RED-EDGE\",\n",
        "        \"NEAR-INFRARED\"\n",
        "    )\n",
        "\n",
        "    class_names = (\n",
        "        \"Seagrass\",\n",
        "        \"Non-Seagrass\",\n",
        "    )\n",
        "\n",
        "    rgb_bands = (\"RED\", \"GREEN\", \"BLUE\")\n",
        "\n",
        "    BAND_SETS = {\"all\": all_band_names, \"rgb\": rgb_bands}\n",
        "\n",
        "    num_classes = 13\n",
        "    time_steps = 3\n",
        "    splits = {\"train\": \"training\", \"val\": \"validation\"}  # Only train and val splits available\n",
        "    col_name = \"chip_id\"\n",
        "    date_columns = [\"first_img_date\", \"middle_img_date\", \"last_img_date\"]\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        data_root: str,\n",
        "        split: str = \"train\",\n",
        "        bands: Sequence[str] = BAND_SETS[\"all\"],\n",
        "        transform: A.Compose | None = None,\n",
        "        no_data_replace: float | None = None,\n",
        "        no_label_replace: int | None = None,\n",
        "        expand_temporal_dimension: bool = True,\n",
        "        reduce_zero_label: bool = True,\n",
        "        use_metadata: bool = False,\n",
        "        metadata_file_name: str = \"chips_df.csv\",\n",
        "    ) -> None:\n",
        "        \"\"\"Constructor\n",
        "\n",
        "        Args:\n",
        "            data_root (str): Path to the data root directory.\n",
        "            split (str): one of 'train' or 'val'.\n",
        "            bands (list[str]): Bands that should be output by the dataset. Defaults to all bands.\n",
        "            transform (A.Compose | None): Albumentations transform to be applied.\n",
        "                Should end with ToTensorV2(). If used through the corresponding data module,\n",
        "                should not include normalization. Defaults to None, which applies ToTensorV2().\n",
        "            no_data_replace (float | None): Replace nan values in input images with this value.\n",
        "                If None, does no replacement. Defaults to None.\n",
        "            no_label_replace (int | None): Replace nan values in label with this value.\n",
        "                If none, does no replacement. Defaults to None.\n",
        "            expand_temporal_dimension (bool): Go from shape (time*channels, h, w) to (channels, time, h, w).\n",
        "                Defaults to True.\n",
        "            reduce_zero_label (bool): Subtract 1 from all labels. Useful when labels start from 1 instead of the\n",
        "                expected 0. Defaults to True.\n",
        "            use_metadata (bool): whether to return metadata info (time and location).\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        if split not in self.splits:\n",
        "            msg = f\"Incorrect split '{split}', please choose one of {self.splits}.\"\n",
        "            raise ValueError(msg)\n",
        "        split_name = self.splits[split]\n",
        "        self.split = split\n",
        "\n",
        "        validate_bands(bands, self.all_band_names)\n",
        "        self.bands = bands\n",
        "        self.band_indices = np.asarray([self.all_band_names.index(b) for b in bands])\n",
        "        self.data_root = Path(data_root)\n",
        "\n",
        "        data_dir = self.data_root / f\"{split_name}_chips\"\n",
        "        self.image_files = sorted(glob.glob(os.path.join(data_dir, \"*_merged.tif\")))\n",
        "        self.segmentation_mask_files = sorted(glob.glob(os.path.join(data_dir, \"*.mask.tif\")))\n",
        "        split_file = self.data_root / f\"{split_name}_data.txt\"\n",
        "\n",
        "        with open(split_file) as f:\n",
        "            split = f.readlines()\n",
        "        valid_files = {rf\"{substring.strip()}\" for substring in split}\n",
        "        self.image_files = filter_valid_files(\n",
        "            self.image_files,\n",
        "            valid_files=valid_files,\n",
        "            ignore_extensions=True,\n",
        "            allow_substring=True,\n",
        "        )\n",
        "        self.segmentation_mask_files = filter_valid_files(\n",
        "            self.segmentation_mask_files,\n",
        "            valid_files=valid_files,\n",
        "            ignore_extensions=True,\n",
        "            allow_substring=True,\n",
        "        )\n",
        "\n",
        "        self.no_data_replace = no_data_replace\n",
        "        self.no_label_replace = no_label_replace\n",
        "        self.reduce_zero_label = reduce_zero_label\n",
        "        self.expand_temporal_dimension = expand_temporal_dimension\n",
        "        self.use_metadata = use_metadata\n",
        "        self.metadata = None\n",
        "        self.metadata_file_name = metadata_file_name\n",
        "        if self.use_metadata:\n",
        "            metadata_file = self.data_root / self.metadata_file_name\n",
        "            self.metadata = pd.read_csv(metadata_file)\n",
        "            self._build_image_metadata_mapping()\n",
        "\n",
        "        # If no transform is given, apply only to transform to torch tensor\n",
        "        self.transform = transform if transform else default_transform\n",
        "\n",
        "    def _build_image_metadata_mapping(self):\n",
        "        \"\"\"Build a mapping from image filenames to metadata indices.\"\"\"\n",
        "        self.image_to_metadata_index = dict()\n",
        "\n",
        "        for idx, image_file in enumerate(self.image_files):\n",
        "            image_filename = Path(image_file).name\n",
        "            image_id = image_filename.replace(\"_merged.tif\", \"\").replace(\".tif\", \"\")\n",
        "            metadata_indices = self.metadata.index[self.metadata[self.col_name] == image_id].tolist()\n",
        "            self.image_to_metadata_index[idx] = metadata_indices[0]\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def _get_date(self, row: pd.Series) -> torch.Tensor:\n",
        "        \"\"\"Extract and format temporal coordinates (T, date) from metadata.\"\"\"\n",
        "        temporal_coords = []\n",
        "        for col in self.date_columns:\n",
        "            date_str = row[col]\n",
        "            date = pd.to_datetime(date_str)\n",
        "            temporal_coords.append([date.year, date.dayofyear - 1])\n",
        "\n",
        "        return torch.tensor(temporal_coords, dtype=torch.float32)\n",
        "\n",
        "    def _get_coords(self, image: DataArray) -> torch.Tensor:\n",
        "        px = image.x.shape[0] // 2\n",
        "        py = image.y.shape[0] // 2\n",
        "\n",
        "        # get center point to reproject to lat/lon\n",
        "        point = image.isel(band=0, x=slice(px, px + 1), y=slice(py, py + 1))\n",
        "        point = point.rio.reproject(\"epsg:4326\")\n",
        "\n",
        "        lat_lon = np.asarray([point.y[0], point.x[0]])\n",
        "\n",
        "        return torch.tensor(lat_lon, dtype=torch.float32)\n",
        "\n",
        "    def __getitem__(self, index: int) -> dict[str, Any]:\n",
        "        image = self._load_file(self.image_files[index], nan_replace=self.no_data_replace)\n",
        "\n",
        "        location_coords, temporal_coords = None, None\n",
        "        if self.use_metadata:\n",
        "            location_coords = self._get_coords(image)\n",
        "            metadata_idx = self.image_to_metadata_index.get(index, None)\n",
        "            if metadata_idx is not None:\n",
        "                row = self.metadata.iloc[metadata_idx]\n",
        "                temporal_coords = self._get_date(row)\n",
        "\n",
        "        # to channels last\n",
        "        image = image.to_numpy()\n",
        "        if self.expand_temporal_dimension:\n",
        "            image = rearrange(image, \"(channels time) h w -> channels time h w\", channels=len(self.bands))\n",
        "        image = np.moveaxis(image, 0, -1)\n",
        "\n",
        "        # filter bands\n",
        "        image = image[..., self.band_indices]\n",
        "\n",
        "        output = {\n",
        "            \"image\": image.astype(np.float32),\n",
        "            \"mask\": self._load_file(\n",
        "                self.segmentation_mask_files[index], nan_replace=self.no_label_replace).to_numpy()[0],\n",
        "        }\n",
        "\n",
        "        if self.reduce_zero_label:\n",
        "            output[\"mask\"] -= 1\n",
        "        if self.transform:\n",
        "            output = self.transform(**output)\n",
        "        output[\"mask\"] = output[\"mask\"].long()\n",
        "\n",
        "        if self.use_metadata:\n",
        "            output[\"location_coords\"] = location_coords\n",
        "            output[\"temporal_coords\"] = temporal_coords\n",
        "\n",
        "        return output\n",
        "\n",
        "    def _load_file(self, path: Path, nan_replace: int | float | None = None) -> DataArray:\n",
        "        data = rioxarray.open_rasterio(path, masked=True)\n",
        "        if nan_replace is not None:\n",
        "            data = data.fillna(nan_replace)\n",
        "        return data\n",
        "\n",
        "    def plot(self, sample: dict[str, Tensor], suptitle: str | None = None) -> Figure:\n",
        "        \"\"\"Plot a sample from the dataset.\n",
        "\n",
        "        Args:\n",
        "            sample: a sample returned by :meth:`__getitem__`\n",
        "            suptitle: optional string to use as a suptitle\n",
        "\n",
        "        Returns:\n",
        "            a matplotlib Figure with the rendered sample\n",
        "        \"\"\"\n",
        "        num_images = self.time_steps + 2\n",
        "\n",
        "        rgb_indices = [self.bands.index(band) for band in self.rgb_bands]\n",
        "        if len(rgb_indices) != 3:\n",
        "            msg = \"Dataset doesn't contain some of the RGB bands\"\n",
        "            raise ValueError(msg)\n",
        "\n",
        "        images = sample[\"image\"]\n",
        "        images = images[rgb_indices, ...]  # Shape: (T, 3, H, W)\n",
        "\n",
        "        processed_images = []\n",
        "        for t in range(self.time_steps):\n",
        "            img = images[t]\n",
        "            img = img.permute(1, 2, 0)\n",
        "            img = img.numpy()\n",
        "            img = clip_image(img)\n",
        "            processed_images.append(img)\n",
        "\n",
        "        mask = sample[\"mask\"].numpy()\n",
        "        if \"prediction\" in sample:\n",
        "            num_images += 1\n",
        "        fig, ax = plt.subplots(1, num_images, figsize=(12, 5), layout=\"compressed\")\n",
        "        ax[0].axis(\"off\")\n",
        "\n",
        "        norm = mpl.colors.Normalize(vmin=0, vmax=self.num_classes - 1)\n",
        "        for i, img in enumerate(processed_images):\n",
        "            ax[i + 1].axis(\"off\")\n",
        "            ax[i + 1].title.set_text(f\"T{i}\")\n",
        "            ax[i + 1].imshow(img)\n",
        "\n",
        "        ax[self.time_steps + 1].axis(\"off\")\n",
        "        ax[self.time_steps + 1].title.set_text(\"Ground Truth Mask\")\n",
        "        ax[self.time_steps + 1].imshow(mask, cmap=\"jet\", norm=norm)\n",
        "\n",
        "        if \"prediction\" in sample:\n",
        "            prediction = sample[\"prediction\"]\n",
        "            ax[self.time_steps + 2].axis(\"off\")\n",
        "            ax[self.time_steps + 2].title.set_text(\"Predicted Mask\")\n",
        "            ax[self.time_steps + 2].imshow(prediction, cmap=\"jet\", norm=norm)\n",
        "\n",
        "        cmap = plt.get_cmap(\"jet\")\n",
        "        legend_data = [[i, cmap(norm(i)), self.class_names[i]] for i in range(self.num_classes)]\n",
        "        handles = [Rectangle((0, 0), 1, 1, color=tuple(v for v in c)) for k, c, n in legend_data]\n",
        "        labels = [n for k, c, n in legend_data]\n",
        "        ax[0].legend(handles, labels, loc=\"center\")\n",
        "\n",
        "        if suptitle is not None:\n",
        "            plt.suptitle(suptitle)\n",
        "\n",
        "        return fig\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from collections.abc import Sequence\n",
        "from typing import Any\n",
        "\n",
        "import albumentations as A\n",
        "from torch import Tensor\n",
        "from torch.utils.data import DataLoader\n",
        "from torchgeo.datamodules import NonGeoDataModule\n",
        "\n",
        "from terratorch.datamodules.generic_pixel_wise_data_module import Normalize\n",
        "from terratorch.datamodules.utils import wrap_in_compose_is_list\n",
        "\n",
        "# statistics calculated from patches using\n",
        "# https://github.com/7yl4r/nasa-prithvi-wetlands/blob/main/py/calculate_chip_statistics.py\n",
        "MEANS = {\n",
        "    \"COASTAL BLUE\": 239.935709,\n",
        "    \"BLUE\": 234.500603,\n",
        "    \"GREEN I\": 284.371003,\n",
        "    \"GREEN II\": 278.806835,\n",
        "    \"YELLOW\": 282.793717,\n",
        "    \"RED\": 237.836126,\n",
        "    \"RED-EDGE\": 419.842239,\n",
        "    \"NEAR-INFRARED\": 1086.378684\n",
        "}\n",
        "\n",
        "STDS = {\n",
        "    \"COASTAL BLUE\": 129.262943,\n",
        "    \"BLUE\": 152.548361,\n",
        "    \"GREEN I\": 172.958055,\n",
        "    \"GREEN II\": 187.841228,\n",
        "    \"YELLOW\": 206.108674,\n",
        "    \"RED\": 210.564642,\n",
        "    \"RED-EDGE\": 245.177366,\n",
        "    \"NEAR-INFRARED\": 451.722659\n",
        "}\n",
        "\n",
        "\n",
        "class MultiTemporalSeagrassClassificationDataModule(NonGeoDataModule):\n",
        "    \"\"\"NonGeo LightningDataModule implementation for multi-temporal seagrass classification.\n",
        "\n",
        "    Based on https://github.com/terrastackai/terratorch/blob/main/terratorch/datamodules/multi_temporal_crop_classification.py\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        data_root: str,\n",
        "        batch_size: int = 4,\n",
        "        num_workers: int = 0,\n",
        "        bands: Sequence[str] = MultiTemporalSeagrassClassification.all_band_names,\n",
        "        train_transform: A.Compose | None | list[A.BasicTransform] = None,\n",
        "        val_transform: A.Compose | None | list[A.BasicTransform] = None,\n",
        "        test_transform: A.Compose | None | list[A.BasicTransform] = None,\n",
        "        predict_transform: A.Compose | None | list[A.BasicTransform] = None,\n",
        "        drop_last: bool = True,\n",
        "        no_data_replace: float | None = 0,\n",
        "        no_label_replace: int | None = -1,\n",
        "        expand_temporal_dimension: bool = True,\n",
        "        reduce_zero_label: bool = True,\n",
        "        use_metadata: bool = False,\n",
        "        metadata_file_name: str = \"chips_df.csv\",\n",
        "        **kwargs: Any,\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Initializes the MultiTemporalCropClassificationDataModule for multi-temporal crop classification.\n",
        "\n",
        "        Args:\n",
        "            data_root (str): Directory containing the dataset.\n",
        "            batch_size (int, optional): Batch size for DataLoaders. Defaults to 4.\n",
        "            num_workers (int, optional): Number of workers for data loading. Defaults to 0.\n",
        "            bands (Sequence[str], optional): List of bands to use. Defaults to MultiTemporalCropClassification.all_band_names.\n",
        "            train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training data.\n",
        "            val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation data.\n",
        "            test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing data.\n",
        "            predict_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for prediction data.\n",
        "            drop_last (bool, optional): Whether to drop the last incomplete batch during training. Defaults to True.\n",
        "            no_data_replace (float | None, optional): Replacement value for missing data. Defaults to 0.\n",
        "            no_label_replace (int | None, optional): Replacement value for missing labels. Defaults to -1.\n",
        "            expand_temporal_dimension (bool, optional): Go from shape (time*channels, h, w) to (channels, time, h, w).\n",
        "                Defaults to True.\n",
        "            reduce_zero_label (bool, optional): Subtract 1 from all labels. Useful when labels start from 1 instead of the\n",
        "                expected 0. Defaults to True.\n",
        "            use_metadata (bool): Whether to return metadata info (time and location).\n",
        "            **kwargs: Additional keyword arguments.\n",
        "        \"\"\"\n",
        "        super().__init__(MultiTemporalSeagrassClassification, batch_size, num_workers, **kwargs)\n",
        "        self.data_root = data_root\n",
        "\n",
        "        self.means = [MEANS[b] for b in bands]\n",
        "        self.stds = [STDS[b] for b in bands]\n",
        "        self.bands = bands\n",
        "        self.train_transform = wrap_in_compose_is_list(train_transform)\n",
        "        self.val_transform = wrap_in_compose_is_list(val_transform)\n",
        "        self.test_transform = wrap_in_compose_is_list(test_transform)\n",
        "        self.predict_transform = wrap_in_compose_is_list(predict_transform)\n",
        "        self.aug = Normalize(self.means, self.stds)\n",
        "        self.drop_last = drop_last\n",
        "        self.no_data_replace = no_data_replace\n",
        "        self.no_label_replace = no_label_replace\n",
        "        self.expand_temporal_dimension = expand_temporal_dimension\n",
        "        self.reduce_zero_label = reduce_zero_label\n",
        "        self.use_metadata = use_metadata\n",
        "        self.metadata_file_name = metadata_file_name\n",
        "\n",
        "    def setup(self, stage: str) -> None:\n",
        "        \"\"\"Set up datasets.\n",
        "\n",
        "        Args:\n",
        "            stage: Either fit, validate, test, or predict.\n",
        "        \"\"\"\n",
        "        if stage in [\"fit\"]:\n",
        "            self.train_dataset = self.dataset_class(\n",
        "                split=\"train\",\n",
        "                data_root=self.data_root,\n",
        "                transform=self.train_transform,\n",
        "                bands=self.bands,\n",
        "                no_data_replace=self.no_data_replace,\n",
        "                no_label_replace=self.no_label_replace,\n",
        "                expand_temporal_dimension = self.expand_temporal_dimension,\n",
        "                reduce_zero_label = self.reduce_zero_label,\n",
        "                use_metadata=self.use_metadata,\n",
        "                metadata_file_name=self.metadata_file_name,\n",
        "            )\n",
        "        if stage in [\"fit\", \"validate\"]:\n",
        "            self.val_dataset = self.dataset_class(\n",
        "                split=\"val\",\n",
        "                data_root=self.data_root,\n",
        "                transform=self.val_transform,\n",
        "                bands=self.bands,\n",
        "                no_data_replace=self.no_data_replace,\n",
        "                no_label_replace=self.no_label_replace,\n",
        "                expand_temporal_dimension = self.expand_temporal_dimension,\n",
        "                reduce_zero_label = self.reduce_zero_label,\n",
        "                use_metadata=self.use_metadata,\n",
        "                metadata_file_name=self.metadata_file_name,\n",
        "            )\n",
        "        if stage in [\"test\"]:\n",
        "            self.test_dataset = self.dataset_class(\n",
        "                split=\"val\",\n",
        "                data_root=self.data_root,\n",
        "                transform=self.test_transform,\n",
        "                bands=self.bands,\n",
        "                no_data_replace=self.no_data_replace,\n",
        "                no_label_replace=self.no_label_replace,\n",
        "                expand_temporal_dimension = self.expand_temporal_dimension,\n",
        "                reduce_zero_label = self.reduce_zero_label,\n",
        "                use_metadata=self.use_metadata,\n",
        "                metadata_file_name=self.metadata_file_name,\n",
        "            )\n",
        "        if stage in [\"predict\"]:\n",
        "            self.predict_dataset = self.dataset_class(\n",
        "                split=\"val\",\n",
        "                data_root=self.data_root,\n",
        "                transform=self.predict_transform,\n",
        "                bands=self.bands,\n",
        "                no_data_replace=self.no_data_replace,\n",
        "                no_label_replace=self.no_label_replace,\n",
        "                expand_temporal_dimension = self.expand_temporal_dimension,\n",
        "                reduce_zero_label = self.reduce_zero_label,\n",
        "                use_metadata=self.use_metadata,\n",
        "                metadata_file_name=self.metadata_file_name,\n",
        "            )\n",
        "\n",
        "    def _dataloader_factory(self, split: str) -> DataLoader[dict[str, Tensor]]:\n",
        "        \"\"\"Implement one or more PyTorch DataLoaders.\n",
        "\n",
        "        Args:\n",
        "            split: Either 'train', 'val', 'test', or 'predict'.\n",
        "\n",
        "        Returns:\n",
        "            A collection of data loaders specifying samples.\n",
        "\n",
        "        Raises:\n",
        "            MisconfigurationException: If :meth:`setup` does not define a\n",
        "                dataset or sampler, or if the dataset or sampler has length 0.\n",
        "        \"\"\"\n",
        "        dataset = self._valid_attribute(f\"{split}_dataset\", \"dataset\")\n",
        "        batch_size = self._valid_attribute(f\"{split}_batch_size\", \"batch_size\")\n",
        "        return DataLoader(\n",
        "            dataset=dataset,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=split == \"train\",\n",
        "            num_workers=self.num_workers,\n",
        "            collate_fn=self.collate_fn,\n",
        "            drop_last=split == \"train\" and self.drop_last,\n",
        "        )"
      ],
      "metadata": {
        "id": "nS9FSZZn5vl8"
      },
      "id": "nS9FSZZn5vl8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "735803b1-a4bf-427f-a1e6-5ac755af33fc",
      "metadata": {
        "id": "735803b1-a4bf-427f-a1e6-5ac755af33fc"
      },
      "outputs": [],
      "source": [
        "datamodule = MultiTemporalSeagrassClassificationDataModule(\n",
        "    batch_size=batch_size,\n",
        "    num_workers=num_workers,\n",
        "    data_root=f\"{dataset_path}\",\n",
        "    train_transform=[\n",
        "        terratorch.datasets.transforms.FlattenTemporalIntoChannels(),  # Required for temporal data\n",
        "        albumentations.D4(), # Random flips and rotation\n",
        "        albumentations.pytorch.transforms.ToTensorV2(),\n",
        "        terratorch.datasets.transforms.UnflattenTemporalFromChannels(n_timesteps=3),\n",
        "    ],\n",
        "    val_transform=None,  # Using ToTensor() by default\n",
        "    test_transform=None,\n",
        "    expand_temporal_dimension=True,\n",
        "    use_metadata=False, # The crop dataset has metadata for location and time\n",
        "    reduce_zero_label=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2af2190c",
      "metadata": {
        "id": "2af2190c"
      },
      "outputs": [],
      "source": [
        "# Setup train and val datasets\n",
        "datamodule.setup(\"fit\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4fdd71a0",
      "metadata": {
        "id": "4fdd71a0"
      },
      "outputs": [],
      "source": [
        "datamodule.batch_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a87ed3b7-f7dc-486d-ac59-cd781a070925",
      "metadata": {
        "id": "a87ed3b7-f7dc-486d-ac59-cd781a070925"
      },
      "outputs": [],
      "source": [
        "# Mean and standard deviation calculated from the training dataset for all 6 bands, and 3 timesteps, for zero mean normalization.\n",
        "# checking for the dataset means and stds\n",
        "datamodule.means, datamodule.stds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08644e71-d82f-426c-b0c1-79026fccb578",
      "metadata": {
        "id": "08644e71-d82f-426c-b0c1-79026fccb578"
      },
      "outputs": [],
      "source": [
        "# checking datasets train split size\n",
        "train_dataset = datamodule.train_dataset\n",
        "len(train_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88b86821-3481-4d92-bdba-246568c66c48",
      "metadata": {
        "id": "88b86821-3481-4d92-bdba-246568c66c48"
      },
      "outputs": [],
      "source": [
        "# checking datasets available bands\n",
        "train_dataset.all_band_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9264de41-ab16-43cc-9ea2-ee51b0969624",
      "metadata": {
        "id": "9264de41-ab16-43cc-9ea2-ee51b0969624"
      },
      "outputs": [],
      "source": [
        "# checking datasets classes\n",
        "train_dataset.class_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a1da2ad-a797-4f4a-ad1a-cd10f9addb01",
      "metadata": {
        "id": "3a1da2ad-a797-4f4a-ad1a-cd10f9addb01"
      },
      "outputs": [],
      "source": [
        "# plotting a few samples\n",
        "for i in range(5):\n",
        "    train_dataset.plot(train_dataset[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7062ddc-a3b7-4378-898c-41abcdf2ee3b",
      "metadata": {
        "id": "b7062ddc-a3b7-4378-898c-41abcdf2ee3b"
      },
      "outputs": [],
      "source": [
        "# checking datasets validation split size\n",
        "val_dataset = datamodule.val_dataset\n",
        "len(val_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ede1c1c6-9f60-4510-a2da-572c55d03f79",
      "metadata": {
        "id": "ede1c1c6-9f60-4510-a2da-572c55d03f79"
      },
      "outputs": [],
      "source": [
        "# checking datasets testing split size\n",
        "datamodule.setup(\"test\")\n",
        "test_dataset = datamodule.test_dataset\n",
        "len(test_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4072e2f849c0df2d",
      "metadata": {
        "id": "4072e2f849c0df2d"
      },
      "source": [
        "# Fine-tune Prithvi\n",
        "\n",
        "Here we setup the fine-tuning including which type of task, which head to use and the model parameters. In this case we are doing segemtation task (you can take a look at this and other downstream tasks here [TerraTorch docs](https://ibm.github.io/terratorch/stable/tasks/)) and using a unet decoder. We also set the numbers of images per label with the \"backbone_num_frames\" parameter to allow us to perform multi-temporal classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae69d39a-857a-4392-b058-0f4b518edf6e",
      "metadata": {
        "id": "ae69d39a-857a-4392-b058-0f4b518edf6e",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "pl.seed_everything(0)\n",
        "\n",
        "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
        "    dirpath=\"../output/multicrop/checkpoints/\",\n",
        "    mode=\"max\",\n",
        "    monitor=\"val/Multiclass_Jaccard_Index\", # Variable to monitor\n",
        "    filename=\"best-{epoch:02d}\",\n",
        ")\n",
        "\n",
        "# Lightning Trainer\n",
        "trainer = pl.Trainer(\n",
        "    accelerator=\"auto\",\n",
        "    strategy=\"auto\",\n",
        "    devices=1, # Lightning multi-gpu often fails in notebooks\n",
        "    precision='bf16-mixed',  # Speed up training\n",
        "    num_nodes=1,\n",
        "    logger=True, # Uses TensorBoard by default\n",
        "    max_epochs=max_epochs,\n",
        "    log_every_n_steps=5,\n",
        "    enable_checkpointing=True,\n",
        "    callbacks=[checkpoint_callback, pl.callbacks.RichProgressBar()],\n",
        "    default_root_dir=\"../output/multicrop\",\n",
        ")\n",
        "\n",
        "# Model\n",
        "model = terratorch.tasks.SemanticSegmentationTask(\n",
        "    model_factory=\"EncoderDecoderFactory\",\n",
        "    model_args={\n",
        "        # Backbone\n",
        "        \"backbone\": prithvi_backbone,\n",
        "        \"backbone_pretrained\": True,\n",
        "        \"backbone_num_frames\": 3,\n",
        "        \"backbone_bands\": [\"BLUE\", \"GREEN\", \"RED\", \"NIR_NARROW\", \"SWIR_1\", \"SWIR_2\"],\n",
        "        \"backbone_coords_encoding\": [], # use [\"time\", \"location\"] for time and location metadata\n",
        "\n",
        "        # Necks\n",
        "        \"necks\": [\n",
        "            {\n",
        "                \"name\": \"SelectIndices\",\n",
        "                # \"indices\": [2, 5, 8, 11]  # 100m model\n",
        "                \"indices\": [5, 11, 17, 23]  # 300m model\n",
        "                # \"indices\": [7, 15, 23, 31]  # 300m model\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"ReshapeTokensToImage\",\n",
        "                \"effective_time_dim\": 3\n",
        "            },\n",
        "            {\"name\": \"LearnedInterpolateToPyramidal\"},\n",
        "        ],\n",
        "\n",
        "        # Decoder\n",
        "        \"decoder\": \"UNetDecoder\",\n",
        "        \"decoder_channels\": [512, 256, 128, 64],\n",
        "\n",
        "        # Head\n",
        "        \"head_dropout\": 0.1,\n",
        "        \"num_classes\": 13,\n",
        "    },\n",
        "\n",
        "    loss=\"ce\",\n",
        "    lr=1e-4,\n",
        "    optimizer=\"AdamW\",\n",
        "    ignore_index=-1,\n",
        "    freeze_backbone=True,  # Speeds up fine-tuning\n",
        "    freeze_decoder=False,\n",
        "    plot_on_val=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27fee1e72be7349",
      "metadata": {
        "id": "27fee1e72be7349"
      },
      "outputs": [],
      "source": [
        "# Training\n",
        "trainer.fit(model, datamodule=datamodule)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d9c27a7aa4f21ea",
      "metadata": {
        "id": "2d9c27a7aa4f21ea"
      },
      "source": [
        "# Test the fine-tuned model\n",
        "\n",
        "Let's gather and specify the relevant files for carrying out testing. Look for your .ckpt file produced during the fine-tuning process here it is in '../output/multicrop/checkpoints/best-epoch=00.ckpt'. We have also provided a model that has been trained on the full dataset so that we can compare it to our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "388aa3db0dc07460",
      "metadata": {
        "id": "388aa3db0dc07460"
      },
      "outputs": [],
      "source": [
        "best_ckpt_path = \"../output/multicrop/checkpoints/best-epoch=00.ckpt\"\n",
        "\n",
        "# Download best model checkpoint fine-tuned on full dataset\n",
        "best_ckpt_100_epoch_path = \"multicrop_best-epoch=76.ckpt\"\n",
        "\n",
        "if not os.path.isfile(best_ckpt_100_epoch_path):\n",
        "    gdown.download(\"https://drive.google.com/uc?id=1cO5a9PmV70j6mvlTc8zH8MnKsRCGbefm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f6c4b0d",
      "metadata": {
        "id": "8f6c4b0d"
      },
      "outputs": [],
      "source": [
        "# calculate test metrics\n",
        "trainer.test(model, datamodule=datamodule, ckpt_path=best_ckpt_100_epoch_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62920a2a",
      "metadata": {
        "id": "62920a2a"
      },
      "outputs": [],
      "source": [
        "# get predictions\n",
        "preds = trainer.predict(model, datamodule=datamodule, ckpt_path=best_ckpt_100_epoch_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22dfc723",
      "metadata": {
        "id": "22dfc723"
      },
      "outputs": [],
      "source": [
        "# get data\n",
        "data_loader = trainer.predict_dataloaders\n",
        "batch = next(iter(data_loader))\n",
        "\n",
        "# plot\n",
        "for i in range(batch_size):\n",
        "    sample = {key: batch[key][i] for key in batch}\n",
        "    sample[\"prediction\"] = preds[0][0][0][i].cpu().numpy()\n",
        "\n",
        "    datamodule.predict_dataset.plot(sample)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0c88e2d5ab78020",
      "metadata": {
        "id": "a0c88e2d5ab78020"
      },
      "source": [
        "# Fine-tuning via CLI\n",
        "\n",
        "We also run the fine-tuning via a [CLI](https://ibm.github.io/terratorch/stable/quick_start/#training-with-lightning-tasks). All parameteres we have specified in the notebook can be put in a [yaml]( ../configs/prithvi_v2_eo_300_tl_unet_multitemporal_crop.yaml), and can be run using the command below. Take a look at the [TerraTorch docs](https://ibm.github.io/terratorch/stable/tutorials/the_yaml_config/) for how to setup the config.\n",
        "\n",
        "You might want to restart the session to free up GPU memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2553ddcb",
      "metadata": {
        "id": "2553ddcb"
      },
      "outputs": [],
      "source": [
        "# First let's get the config file from github.com.\n",
        "!git init\n",
        "!git remote add origin https://github.com/IBM/ML4EO-workshop-2025.git\n",
        "!git fetch --all\n",
        "!git checkout origin/main -- \"Prithvi-EO/configs/prithvi_v2_eo_300_tl_unet_multitemporal_crop.yaml\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bdbf05ebc81b9998",
      "metadata": {
        "id": "bdbf05ebc81b9998"
      },
      "outputs": [],
      "source": [
        "# Run fine-tuning\n",
        "!terratorch fit -c \"Prithvi-EO/configs/prithvi_v2_eo_300_tl_unet_multitemporal_crop.yaml\""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}