{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7009d02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Includes work derived from International Business Machines\n",
    "# `prithvi_v2_eo_300_tl_unet_multitemporal_crop.ipynb``\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# You may obtain a copy of the License at\n",
    "#  http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# All rights reserved for this derived work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5ca9883e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully built nasa-prithvi-wetlands\n",
      "Installing collected packages: nasa-prithvi-wetlands\n",
      "  Attempting uninstall: nasa-prithvi-wetlands\n",
      "    Found existing installation: nasa-prithvi-wetlands 0.1.0\n",
      "    Uninstalling nasa-prithvi-wetlands-0.1.0:\n",
      "      Successfully uninstalled nasa-prithvi-wetlands-0.1.0\n",
      "Successfully installed nasa-prithvi-wetlands-0.1.0\n"
     ]
    }
   ],
   "source": [
    "# Bash command to install dependencies from pyproject.toml with `pip``;\n",
    "# ouput is piped to `tail`` to limit length of text printed below.\n",
    "!pip install -e . | tail -n 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bacc318390456b",
   "metadata": {},
   "source": [
    "## Overview\n",
    "This notebook focuses on fine-tuning the [Prithvi EO v2.0 model](https://huggingface.co/collections/ibm-nasa-geospatial/prithvi-for-earth-observation-6740a7a81883466bf41d93d6) to classify seagrass.\n",
    "\n",
    "This notebook:\n",
    "1. Is intended to be run on Google Colab.\n",
    "2. Uses Terratorch to fine-tune Prithvi EO v2.0 300m.\n",
    "3. Uses a seagrass patch dataset for fine-tuning derived from the IMaRS SIMM Seagrass Project.\n",
    "4. Uses fine-tuned model for inference.\n",
    "\n",
    "You may want to take this opportunity to double check you're using GPUs on Google Colab before proceeding any further. We have tested this notebook using T4 GPU on the free colab account.\n",
    "\n",
    "## Setup\n",
    "1. Install terratorch\n",
    "\n",
    "To install the necessary packages, execute the cell below. This will take a few minutes. Once the installation process is done, a window will pop up to ask you to restart the session. This is normal and you should proceed to restart using the interface in the pop up window. Once the session has restarted, its important that you ignore the cell below, and go straight to section 0.1.3.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c25f3b",
   "metadata": {},
   "source": [
    "2. Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2e8c1961-311b-49e0-b6ea-2867b44cb47a",
   "metadata": {
    "id": "2e8c1961-311b-49e0-b6ea-2867b44cb47a"
   },
   "outputs": [],
   "source": [
    "\n",
    "import albumentations\n",
    "import gdown\n",
    "import lightning.pytorch as pl\n",
    "import os\n",
    "import terratorch\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from terratorch.datamodules import MultiTemporalCropClassificationDataModule\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14cbc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download tuning dataset .bz2 from Google Drive and place in `dataset_path`` directory.\n",
    "dataset_path = \"data/tuning_patches\"\n",
    "fname = \"seagrass_tuning_patches.tar.bz2\"\n",
    "gdown.download(\n",
    "https://drive.google.com/file/d//view?usp=sharing    \n",
    "    \"https://drive.google.com/uc?id=1VWl2mkTTAG3ih741n3S5UDUj71ygNKDp\",\n",
    "    os.path.join(dataset_path, fname),\n",
    "    quiet=False,\n",
    ")\n",
    "\n",
    "# uzip the downloaded .bz2 file\n",
    "!bzip2 -xvjf {os.path.join(dataset_path, fname)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917b65b8e7cd7d65",
   "metadata": {},
   "source": [
    "A tuning dataset should now be in the dataset_path directory.\n",
    "The dataset used is derived from the IMaRS SIMM Seagrasss Project.\n",
    "For methods to generate the patches, see the `py/generate_seagrass_patches.py` script in the repo.\n",
    "\n",
    "\n",
    "\n",
    "The patches need to be split into training and validation sets.\n",
    "The sets are specified using a `.txt` file listing the patch file names for each set.\n",
    "Example `training_chips.txt`:\n",
    "\n",
    "```\n",
    "chip_257_266\n",
    "chip_328_501\n",
    "chip_171_477\n",
    "chip_236_281\n",
    "chip_134_482\n",
    "chip_120_493\n",
    "chip_161_390\n",
    "```\n",
    "\n",
    "The training and validation patches are expected to be in `training_chips` and `validation_chips` subdirectories of the dataset path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3t-YKKUztjXn",
   "metadata": {
    "id": "3t-YKKUztjXn",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "20332275",
   "metadata": {},
   "source": [
    "4. Truncate the dataset for demonstration purposes. Reducing the training dataset to a third of the original size means that model training takes only a few minutes with the resources available during the workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4517aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_truncation = 800\n",
    "validation_data_trunction = 4\n",
    "with open(f\"{dataset_path}/training_data.txt\", \"r\") as f:\n",
    "      training_data_list = f.readlines()\n",
    "truncated = training_data_list[0:training_data_truncation]\n",
    "with open(f\"{dataset_path}/training_data.txt\", \"w\") as f:\n",
    "    for i in truncated:\n",
    "        f.write(i)\n",
    "\n",
    "with open(f\"{dataset_path}/validation_data.txt\", \"r\") as f:\n",
    "      training_data_list = f.readlines()\n",
    "truncated = training_data_list[0:validation_data_trunction]\n",
    "with open(f\"{dataset_path}/validation_data.txt\", \"w\") as f:\n",
    "    for i in truncated:\n",
    "        f.write(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ba4d58-8ff6-4f9c-bfb1-a70376f80494",
   "metadata": {
    "id": "35ba4d58-8ff6-4f9c-bfb1-a70376f80494"
   },
   "source": [
    "## Dataset Details\n",
    "\n",
    "Lets start with analysing the dataset. \n",
    "\n",
    "Please note: we have also set the batch_size parameter to 4 and max_epochs to 1 to avoid running out of memory or runtime for users of the free tier colab compute resources. This is enough to demonstrate the entire workflow to the user, but may not result in the best performance. It'll be best to find additional compute resources and increase batch_size and max_epochs in the downloaded config file for improved performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ddd7d83440895e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access 'data/tuning_patches/training_chips': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# Each merged sample includes the stacked bands of three time steps\n",
    "!ls \"{dataset_path}/training_chips\" | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd1487c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify parameters to select the batch size, number of workers, model backbone and epochs ahead of initalizing the MultiTemporalCropClassificationDataModule class for multi-temporal crop classification. \n",
    "batch_size = 4\n",
    "num_workers = 2\n",
    "prithvi_backbone = \"prithvi_eo_v2_300_tl\" # Model can be either prithvi_eo_v1_100, prithvi_eo_v2_300, prithvi_eo_v2_300_tl, prithvi_eo_v2_600, prithvi_eo_v2_600_tl\n",
    "\n",
    "# Total number of epochs the training will run for.\n",
    "max_epochs =  1 # Use 1 epoch for demos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91025bb8",
   "metadata": {},
   "source": [
    "#### Initialise the Datamodules class \n",
    "\n",
    "A Datamodule is a shareable, reusable class that encapsulates all the steps needed to process the data. Here we are using an adjusted dataset class for this dataset (general dataset class could be used as well). To learn more about MultiTemporalCropClassificationDataModule, take a look at the [TerraTorch docs](https://ibm.github.io/terratorch/stable/datamodules/?h=multitemporalcropclassificationdatamodule#terratorch.datamodules.multi_temporal_crop_classification.MultiTemporalCropClassificationDataModule)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735803b1-a4bf-427f-a1e6-5ac755af33fc",
   "metadata": {
    "id": "735803b1-a4bf-427f-a1e6-5ac755af33fc"
   },
   "outputs": [],
   "source": [
    "datamodule = MultiTemporalCropClassificationDataModule(\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    data_root=f\"{dataset_path}\",\n",
    "    train_transform=[\n",
    "        terratorch.datasets.transforms.FlattenTemporalIntoChannels(),  # Required for temporal data\n",
    "        albumentations.D4(), # Random flips and rotation\n",
    "        albumentations.pytorch.transforms.ToTensorV2(),\n",
    "        terratorch.datasets.transforms.UnflattenTemporalFromChannels(n_timesteps=3),\n",
    "    ],\n",
    "    val_transform=None,  # Using ToTensor() by default\n",
    "    test_transform=None,\n",
    "    expand_temporal_dimension=True,\n",
    "    use_metadata=False, # The crop dataset has metadata for location and time\n",
    "    reduce_zero_label=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af2190c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup train and val datasets\n",
    "datamodule.setup(\"fit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdd71a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87ed3b7-f7dc-486d-ac59-cd781a070925",
   "metadata": {
    "id": "a87ed3b7-f7dc-486d-ac59-cd781a070925"
   },
   "outputs": [],
   "source": [
    "# Mean and standard deviation calculated from the training dataset for all 6 bands, and 3 timesteps, for zero mean normalization.\n",
    "# checking for the dataset means and stds\n",
    "datamodule.means, datamodule.stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08644e71-d82f-426c-b0c1-79026fccb578",
   "metadata": {
    "id": "08644e71-d82f-426c-b0c1-79026fccb578"
   },
   "outputs": [],
   "source": [
    "# checking datasets train split size\n",
    "train_dataset = datamodule.train_dataset\n",
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b86821-3481-4d92-bdba-246568c66c48",
   "metadata": {
    "id": "88b86821-3481-4d92-bdba-246568c66c48"
   },
   "outputs": [],
   "source": [
    "# checking datasets available bands\n",
    "train_dataset.all_band_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9264de41-ab16-43cc-9ea2-ee51b0969624",
   "metadata": {
    "id": "9264de41-ab16-43cc-9ea2-ee51b0969624"
   },
   "outputs": [],
   "source": [
    "# checking datasets classes\n",
    "train_dataset.class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1da2ad-a797-4f4a-ad1a-cd10f9addb01",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "3a1da2ad-a797-4f4a-ad1a-cd10f9addb01",
    "outputId": "9c948b7c-e02b-4980-a142-b36bcb51a8e4"
   },
   "outputs": [],
   "source": [
    "# plotting a few samples\n",
    "for i in range(5):\n",
    "    train_dataset.plot(train_dataset[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7062ddc-a3b7-4378-898c-41abcdf2ee3b",
   "metadata": {
    "id": "b7062ddc-a3b7-4378-898c-41abcdf2ee3b"
   },
   "outputs": [],
   "source": [
    "# checking datasets validation split size\n",
    "val_dataset = datamodule.val_dataset\n",
    "len(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede1c1c6-9f60-4510-a2da-572c55d03f79",
   "metadata": {
    "id": "ede1c1c6-9f60-4510-a2da-572c55d03f79"
   },
   "outputs": [],
   "source": [
    "# checking datasets testing split size\n",
    "datamodule.setup(\"test\")\n",
    "test_dataset = datamodule.test_dataset\n",
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4072e2f849c0df2d",
   "metadata": {},
   "source": [
    "# Fine-tune Prithvi\n",
    "\n",
    "Here we setup the fine-tuning including which type of task, which head to use and the model parameters. In this case we are doing segemtation task (you can take a look at this and other downstream tasks here [TerraTorch docs](https://ibm.github.io/terratorch/stable/tasks/)) and using a unet decoder. We also set the numbers of images per label with the \"backbone_num_frames\" parameter to allow us to perform multi-temporal classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae69d39a-857a-4392-b058-0f4b518edf6e",
   "metadata": {
    "id": "ae69d39a-857a-4392-b058-0f4b518edf6e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pl.seed_everything(0)\n",
    "\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    dirpath=\"../output/multicrop/checkpoints/\",\n",
    "    mode=\"max\",\n",
    "    monitor=\"val/Multiclass_Jaccard_Index\", # Variable to monitor\n",
    "    filename=\"best-{epoch:02d}\",\n",
    ")\n",
    "\n",
    "# Lightning Trainer\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=\"auto\",\n",
    "    strategy=\"auto\",\n",
    "    devices=1, # Lightning multi-gpu often fails in notebooks\n",
    "    precision='bf16-mixed',  # Speed up training\n",
    "    num_nodes=1,\n",
    "    logger=True, # Uses TensorBoard by default\n",
    "    max_epochs=max_epochs,\n",
    "    log_every_n_steps=5,\n",
    "    enable_checkpointing=True,\n",
    "    callbacks=[checkpoint_callback, pl.callbacks.RichProgressBar()],\n",
    "    default_root_dir=\"../output/multicrop\",\n",
    ")\n",
    "\n",
    "# Model\n",
    "model = terratorch.tasks.SemanticSegmentationTask(\n",
    "    model_factory=\"EncoderDecoderFactory\",\n",
    "    model_args={\n",
    "        # Backbone\n",
    "        \"backbone\": prithvi_backbone,\n",
    "        \"backbone_pretrained\": True,\n",
    "        \"backbone_num_frames\": 3,\n",
    "        \"backbone_bands\": [\"BLUE\", \"GREEN\", \"RED\", \"NIR_NARROW\", \"SWIR_1\", \"SWIR_2\"],\n",
    "        \"backbone_coords_encoding\": [], # use [\"time\", \"location\"] for time and location metadata\n",
    "        \n",
    "        # Necks \n",
    "        \"necks\": [\n",
    "            {\n",
    "                \"name\": \"SelectIndices\",\n",
    "                # \"indices\": [2, 5, 8, 11]  # 100m model\n",
    "                \"indices\": [5, 11, 17, 23]  # 300m model\n",
    "                # \"indices\": [7, 15, 23, 31]  # 300m model\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"ReshapeTokensToImage\",\n",
    "                \"effective_time_dim\": 3\n",
    "            },\n",
    "            {\"name\": \"LearnedInterpolateToPyramidal\"},\n",
    "        ],\n",
    "        \n",
    "        # Decoder\n",
    "        \"decoder\": \"UNetDecoder\",\n",
    "        \"decoder_channels\": [512, 256, 128, 64],\n",
    "        \n",
    "        # Head\n",
    "        \"head_dropout\": 0.1,\n",
    "        \"num_classes\": 13,\n",
    "    },\n",
    "\n",
    "    loss=\"ce\",\n",
    "    lr=1e-4,\n",
    "    optimizer=\"AdamW\",\n",
    "    ignore_index=-1,\n",
    "    freeze_backbone=True,  # Speeds up fine-tuning\n",
    "    freeze_decoder=False,\n",
    "    plot_on_val=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fee1e72be7349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "trainer.fit(model, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9c27a7aa4f21ea",
   "metadata": {},
   "source": [
    "# Test the fine-tuned model\n",
    "\n",
    "Let's gather and specify the relevant files for carrying out testing. Look for your .ckpt file produced during the fine-tuning process here it is in '../output/multicrop/checkpoints/best-epoch=00.ckpt'. We have also provided a model that has been trained on the full dataset so that we can compare it to our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388aa3db0dc07460",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_ckpt_path = \"../output/multicrop/checkpoints/best-epoch=00.ckpt\"\n",
    "\n",
    "# Download best model checkpoint fine-tuned on full dataset\n",
    "best_ckpt_100_epoch_path = \"multicrop_best-epoch=76.ckpt\"\n",
    "\n",
    "if not os.path.isfile(best_ckpt_100_epoch_path):\n",
    "    gdown.download(\"https://drive.google.com/uc?id=1cO5a9PmV70j6mvlTc8zH8MnKsRCGbefm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6c4b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate test metrics\n",
    "trainer.test(model, datamodule=datamodule, ckpt_path=best_ckpt_100_epoch_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62920a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions\n",
    "preds = trainer.predict(model, datamodule=datamodule, ckpt_path=best_ckpt_100_epoch_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22dfc723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data \n",
    "data_loader = trainer.predict_dataloaders\n",
    "batch = next(iter(data_loader))\n",
    "\n",
    "# plot\n",
    "for i in range(batch_size):\n",
    "    sample = {key: batch[key][i] for key in batch}\n",
    "    sample[\"prediction\"] = preds[0][0][0][i].cpu().numpy()\n",
    "\n",
    "    datamodule.predict_dataset.plot(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c88e2d5ab78020",
   "metadata": {},
   "source": [
    "# Fine-tuning via CLI\n",
    "\n",
    "We also run the fine-tuning via a [CLI](https://ibm.github.io/terratorch/stable/quick_start/#training-with-lightning-tasks). All parameteres we have specified in the notebook can be put in a [yaml]( ../configs/prithvi_v2_eo_300_tl_unet_multitemporal_crop.yaml), and can be run using the command below. Take a look at the [TerraTorch docs](https://ibm.github.io/terratorch/stable/tutorials/the_yaml_config/) for how to setup the config.\n",
    "\n",
    "You might want to restart the session to free up GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2553ddcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's get the config file from github.com.\n",
    "!git init\n",
    "!git remote add origin https://github.com/IBM/ML4EO-workshop-2025.git\n",
    "!git fetch --all\n",
    "!git checkout origin/main -- \"Prithvi-EO/configs/prithvi_v2_eo_300_tl_unet_multitemporal_crop.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbf05ebc81b9998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run fine-tuning\n",
    "!terratorch fit -c \"Prithvi-EO/configs/prithvi_v2_eo_300_tl_unet_multitemporal_crop.yaml\""
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
